# -*- coding: utf-8 -*-
"""VIP Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZulD6oFcwvssSyH1Y8FIvKC4aCRj451
"""

import cv2
import keras
import json
import shutil
import math
import random
import numpy as np
import pandas as pd 
from glob import glob
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.models import Sequential 
from keras.layers.normalization import BatchNormalization
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D

from google.colab import drive
drive.mount("/content/gdrive/", force_remount=True)

train = ImageDataGenerator(rescale=1/255)
valid = ImageDataGenerator(rescale =1/255)
image_size = 224

train_dataset = train.flow_from_directory('/content/gdrive/My Drive/MMU Stuff/VIP/Project/train/',
                                          target_size=(image_size, image_size),
                                          batch_size=20,
                                          class_mode ='categorical')

valid_dataset = train.flow_from_directory('/content/gdrive/My Drive/MMU Stuff/VIP/Project/valid/',
                                          target_size=(image_size, image_size),
                                          batch_size=20,
                                          class_mode ='categorical')

train_dir = '/content/gdrive/My Drive/MMU Stuff/VIP/Project/train/'
valid_dir = '/content/gdrive/My Drive/MMU Stuff/VIP/Project/valid/'



image_size = 224
nTrain = 600
nVal = 150

## Data Generator 
EPOCHS = 100
BATCH_SIZE = 32
image_height = 227
image_width = 227


train_datagen = ImageDataGenerator(
                  rescale=1./255,
                  #rotation_range=10,
                  #width_shift_range=0.1,
                  #height_shift_range=0.1,
                  brightness_range = [0.9,1.1], 
                  #shear_range=0.1,
                  zoom_range=0.1)

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(image_height, image_width),
                                                    color_mode="rgb",
                                                    batch_size=BATCH_SIZE,
                                                    seed=1,
                                                    shuffle=True,
                                                    class_mode="categorical")

valid_datagen = ImageDataGenerator(rescale=1.0/255.0)
valid_generator = valid_datagen.flow_from_directory(valid_dir,
                                                    target_size=(image_height, image_width),
                                                    color_mode="rgb",
                                                    batch_size=BATCH_SIZE,
                                                    seed=7,
                                                    shuffle=True,
                                                    class_mode="categorical"
                                                    )
train_num = train_generator.samples
valid_num = valid_generator.samples

# Another AN   ( 2:50AM executed completed with graph )

class AlexNet(Sequential):
    def __init__(self, input_shape, num_classes):
        super().__init__()

        self.add(Conv2D(96, kernel_size=(11,11), strides= 4,
                        padding= 'valid', activation= 'relu',
                        input_shape= input_shape))
        self.add(MaxPooling2D(pool_size=(2,2), strides= (2,2),
                              padding= 'valid'))

        self.add(Conv2D(256, kernel_size=(11,11), strides= (1,1),
                        padding= 'valid', activation= 'relu',
                       ))
        self.add(MaxPooling2D(pool_size=(2,2), strides= (2,2),
                              padding= 'valid')) 

        self.add(Conv2D(384, kernel_size=(3,3), strides= (1,1),
                        padding= 'valid', activation= 'relu',
                        ))

        self.add(Conv2D(384, kernel_size=(3,3), strides= (1,1),
                        padding= 'valid', activation= 'relu',
                        ))

        self.add(Conv2D(256, kernel_size=(3,3), strides= (1,1),
                        padding= 'valid', activation= 'relu',
                        ))

        self.add(MaxPooling2D(pool_size=(2,2), strides= (2,2),
                              padding= 'valid'))

        self.add(Flatten())
        self.add(Dense(4096, activation= 'relu'))
        self.add(Dropout(0.4))
        self.add(Dense(4096, activation= 'relu'))
        self.add(Dropout(0.4))
        self.add(Dense(1000, activation= 'relu'))
        self.add(Dropout(0.4))
        self.add(Dense(num_classes, activation= 'softmax'))

        self.compile(optimizer= tf.keras.optimizers.Adam(0.001),
                    loss='categorical_crossentropy',
                    metrics=['accuracy'])

num_classes = 2

model = AlexNet((227, 227, 3), num_classes)

model.summary()

history = model.fit(train_generator,
                    epochs=50,
                    steps_per_epoch=train_num // BATCH_SIZE,
                    validation_data=valid_generator,
                    validation_steps=valid_num // BATCH_SIZE
                    )

EPOCHS = 100
BATCH_SIZE = 32
image_height = 227
image_width = 227
train_dir = '/content/gdrive/My Drive/MMU Stuff/VIP/Project/train/'
valid_dir = '/content/gdrive/My Drive/MMU Stuff/VIP/Project/valid/'


train_datagen = ImageDataGenerator(
                  rescale=1./255,
                  rotation_range=10,
                  width_shift_range=0.1,
                  height_shift_range=0.1,
                  shear_range=0.1,
                  zoom_range=0.1)

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(image_height, image_width),
                                                    color_mode="rgb",
                                                    batch_size=BATCH_SIZE,
                                                    seed=1,
                                                    shuffle=True,
                                                    class_mode="categorical")

valid_datagen = ImageDataGenerator(rescale=1.0/255.0)
valid_generator = valid_datagen.flow_from_directory(valid_dir,
                                                    target_size=(image_height, image_width),
                                                    color_mode="rgb",
                                                    batch_size=BATCH_SIZE,
                                                    seed=7,
                                                    shuffle=True,
                                                    class_mode="categorical"
                                                    )

train_num = train_generator.samples
valid_num = valid_generator.samples

from keras import optimizers
model.compile(loss='categorical_crossentropy',
             optimizer=optimizers.RMSprop(lr=1e-4),
             metrics=['acc'])

# Plot the accuracy and loss curves
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

# Create a generator for prediction
validation_generator = valid_datagen.flow_from_directory(
        valid_dir,
        target_size=(image_size, image_size),
        batch_size=32,
        class_mode='categorical',
        shuffle=False)

# Get the filenames from the generator
fnames = validation_generator.filenames

# Get the ground truth from generator
ground_truth = validation_generator.classes

# Get the label to class mapping from the generator
label2index = validation_generator.class_indices

# Getting the mapping from class index to class label
idx2label = dict((v,k) for k,v in label2index.items())

# Get the predictions from the model using the generator
predictions = model.predict(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)
predicted_classes = np.argmax(predictions,axis=1)

errors = np.where(predicted_classes != ground_truth)[0]
print("No of errors = {}/{}".format(len(errors),validation_generator.samples))

# Show the errors
for i in range(len(errors)):
    pred_class = np.argmax(predictions[errors[i]])
    pred_label = idx2label[pred_class]
    
    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(
        fnames[errors[i]].split('/')[0],
        pred_label,
        predictions[errors[i]][pred_class])
    
    original = load_img('{}/{}'.format(valid_dir,fnames[errors[i]]))
    plt.figure(figsize=[7,7])
    plt.axis('off')
    plt.title(title)
    plt.imshow(original)
    plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns 

ground_truth = valid_generator.classes
predicted_classes = np.argmax(predictions,axis=1)
cf = confusion_matrix(ground_truth,predicted_classes)
group_counts = ['{0:0.0f}'.format(value) for value in cf.flatten()]
class_count = np.array(group_counts).reshape(cf.shape)
plt.figure(figsize=(10,10))
sns.heatmap(cf,annot=class_count,xticklabels=valid_generator.class_indices.keys(),yticklabels=valid_generator.class_indices.keys(),fmt ='',cmap="Blues")

from keras.applications import VGG16
from keras import models
from keras import layers
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator, load_img

train_dir = '/content/gdrive/My Drive/MMU Stuff/VIP/Project/train/'
valid_dir = '/content/gdrive/My Drive/MMU Stuff/VIP/Project/valid/'

 
image_size = 224
nTrain = 600
nVal = 150

#Load the VGG model
vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))

# Freeze all layers except the last 4 layers
for layer in vgg_conv.layers[:-4]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in vgg_conv.layers:

# Create the model
  modelvgg = models.Sequential()

# Add the vgg convolutional base model
modelvgg.add(vgg_conv)

# Add new layers
modelvgg.add(layers.Flatten())
modelvgg.add(layers.Dense(1024, activation='relu'))
modelvgg.add(layers.Dropout(0.5))
modelvgg.add(layers.Dense(2, activation='softmax'))

# Show a summary of the model. Check the number of trainable parameters
modelvgg.summary()

# Change the batchsize according to your system RAM
train_batchsize = 100
val_batchsize = 10
image_height = 227
image_width = 227
BATCH_SIZE = 32

# Data Generator for Training & Validation data
train_datagen = ImageDataGenerator(
                  rescale=1./255,
                  rotation_range=10,
                  width_shift_range=0.1,
                  height_shift_range=0.1,
                  shear_range=0.1,
                  zoom_range=0.1)

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(image_height, image_width),
                                                    color_mode="rgb",
                                                    batch_size=BATCH_SIZE,
                                                    seed=1,
                                                    shuffle=True,
                                                    class_mode="categorical")

valid_datagen = ImageDataGenerator(rescale=1.0/255.0)
valid_generator = valid_datagen.flow_from_directory(valid_dir,
                                                    target_size=(image_height, image_width),
                                                    color_mode="rgb",
                                                    batch_size=BATCH_SIZE,
                                                    seed=7,
                                                    shuffle=True,
                                                    class_mode="categorical")

train_num = train_generator.samples
valid_num = valid_generator.samples

modelvgg.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4), #lower
              metrics=['acc'])

historyvgg = modelvgg.fit(
      train_generator,
      steps_per_epoch=train_generator.samples/BATCH_SIZE ,
      epochs=30,
      validation_data=validation_generator,
      validation_steps=validation_generator.samples/BATCH_SIZE,
      verbose=1)

modelvgg.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])

# Plot the accuracy and loss curves
acc = historyvgg.history['acc']
val_acc = historyvgg.history['val_acc']
loss = historyvgg.history['loss']
val_loss = historyvgg.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

# Create a generator for prediction
validation_generator = valid_datagen.flow_from_directory(
        valid_dir,
        target_size=(image_size, image_size),
        batch_size=val_batchsize,
        class_mode='categorical',
        shuffle=False)

# Get the filenames from the generator
fnames = validation_generator.filenames

# Get the ground truth from generator
ground_truth = validation_generator.classes

# Get the label to class mapping from the generator
label2index = validation_generator.class_indices

# Getting the mapping from class index to class label
idx2label = dict((v,k) for k,v in label2index.items())

# Get the predictions from the model using the generator
predictions = modelvgg.predict(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)
predicted_classes = np.argmax(predictions,axis=1)

errors = np.where(predicted_classes != ground_truth)[0]
print("No of errors = {}/{}".format(len(errors),validation_generator.samples))

# Show the errors
for i in range(len(errors)):
    pred_class = np.argmax(predictions[errors[i]])
    pred_label = idx2label[pred_class]
    
    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(
        fnames[errors[i]].split('/')[0],
        pred_label,
        predictions[errors[i]][pred_class])
    
    original = load_img('{}/{}'.format(valid_dir,fnames[errors[i]]))
    plt.figure(figsize=[7,7])
    plt.axis('off')
    plt.title(title)
    plt.imshow(original)
    plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns 

ground_truth = valid_generator.classes
predicted_classes = np.argmax(predictions,axis=1)
cf = confusion_matrix(ground_truth,predicted_classes)
group_counts = ['{0:0.0f}'.format(value) for value in cf.flatten()]
class_count = np.array(group_counts).reshape(cf.shape)
plt.figure(figsize=(10,10))
sns.heatmap(cf,annot=class_count,xticklabels=valid_generator.class_indices.keys(),yticklabels=valid_generator.class_indices.keys(),fmt ='',cmap="Blues")

"""#Other training models

"""

from keras.applications import VGG19
from keras import models
from keras import layers
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator, load_img

#Load the VGG model
vgg2 = VGG19(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))

# Freeze all layers except the last 4 layers
for layer in vgg2.layers[:-4]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in vgg2.layers:

# Create the model
  modelvgg2 = models.Sequential()

# Add the vgg convolutional base model
modelvgg2.add(vgg2)

# Add new layers
modelvgg2.add(layers.Flatten())
modelvgg2.add(layers.Dense(1024, activation='relu'))
modelvgg2.add(layers.Dropout(0.5))
modelvgg2.add(layers.Dense(2, activation='softmax'))

# Show a summary of the model. Check the number of trainable parameters
modelvgg2.summary()

modelvgg2.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])

historyvgg2 = modelvgg2.fit(
      train_generator,
      steps_per_epoch=train_generator.samples/BATCH_SIZE ,
      epochs=30,
      validation_data=validation_generator,
      validation_steps=validation_generator.samples/BATCH_SIZE,
      verbose=1)

# Plot the accuracy and loss curves
acc = historyvgg2.history['acc']
val_acc = historyvgg2.history['val_acc']
loss = historyvgg2.history['loss']
val_loss = historyvgg2.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

# Create a generator for prediction
validation_generator = valid_datagen.flow_from_directory(
        valid_dir,
        target_size=(image_size, image_size),
        batch_size=val_batchsize,
        class_mode='categorical',
        shuffle=False)

# Get the filenames from the generator
fnames = validation_generator.filenames

# Get the ground truth from generator
ground_truth = validation_generator.classes

# Get the label to class mapping from the generator
label2index = validation_generator.class_indices

# Getting the mapping from class index to class label
idx2label = dict((v,k) for k,v in label2index.items())

# Get the predictions from the model using the generator
predictions = modelvgg2.predict(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)
predicted_classes = np.argmax(predictions,axis=1)

errors = np.where(predicted_classes != ground_truth)[0]
print("No of errors = {}/{}".format(len(errors),validation_generator.samples))

# Show the errors
for i in range(len(errors)):
    pred_class = np.argmax(predictions[errors[i]])
    pred_label = idx2label[pred_class]
    
    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(
        fnames[errors[i]].split('/')[0],
        pred_label,
        predictions[errors[i]][pred_class])
    
    original = load_img('{}/{}'.format(valid_dir,fnames[errors[i]]))
    plt.figure(figsize=[7,7])
    plt.axis('off')
    plt.title(title)
    plt.imshow(original)
    plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns 

ground_truth = valid_generator.classes
predicted_classes = np.argmax(predictions,axis=1)
cf = confusion_matrix(ground_truth,predicted_classes)
group_counts = ['{0:0.0f}'.format(value) for value in cf.flatten()]
class_count = np.array(group_counts).reshape(cf.shape)
plt.figure(figsize=(10,10))
sns.heatmap(cf,annot=class_count,xticklabels=valid_generator.class_indices.keys(),yticklabels=valid_generator.class_indices.keys(),fmt ='',cmap="Blues")

#DeepID
import numpy as np
import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.callbacks import History

class Model(object):
    def __init__(self, nb_epoch, batch_size, lr): 
        self.nb_epoch = nb_epoch
        self.batch_size = batch_size
        self.lr = lr
        self.model = None
        self.result = None
    
    def cnnModel(self):
        pass

    def train(self, X_train, Y_train, X_test, Y_test):
        model = self.model
        model.compile(loss='categorical_crossentropy', optimizer='adadelta',\
                     metrics=["accuracy"])

        self.result = model.fit(X_train, Y_train, batch_size=self.batch_size,
                               nb_epoch=self.nb_epoch, verbose=2,
                               validation_data=(X_test, Y_test), shuffle=True)

    def predict(self, x):
        return self.model.predict(x)

    def predict_classes(self, X):
        return self.model.predict_classes(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def save_weights(self):
        print('Saving best parameters...')
        self.model.save_weights('FaceCNN_weights.h5', overwrite=True)

    def plot_results(self, val, err, ylabel):
        if (val != 'val_loss' and val != 'val_acc')\
           or (err != 'acc' and err != 'loss'):
            raise ValueError('Invalid key values. {} or {}'.format(val, err))
        result = self.result
        plt.figure()
        plt.plot(result.epoch, result.history[err], label=err)
        plt.plot(result.epoch, result.history[val], label=val)
        plt.scatter(result.epoch, result.history[err])
        plt.scatter(result.epoch, result.history[val])
        plt.legend(loc='under right')
        plt.ylabel(ylabel)
        plt.xlabel('Epochs (one pass through training data)')
        plt.savefig(err+'.jpg')

    def save_accuracy(self):
        self.plot_results('val_acc', 'acc', 'Accuracy (no. images classified correctly)')

    def save_loss(self):
        self.plot_results('val_loss', 'loss', 'Loss')

class FaceCNN(Model):
    def __init__(self, nb_class, lr, nb_epoch=0, batch_size=0, weights_path=None):
        Model.__init__(self, nb_epoch, batch_size, lr)
        self.model = self.cnnModel(nb_class, weights_path)
        if weights_path:
            print('loading weights...')
            self.model.compile(optimizer='adadelta', \
                               loss='categorical_crossentropy')

    def cnnModel(self, nb_class, weights_path=None):
        model = Sequential()
        model.add(Convolution2D(32, 3, 3,
                border_mode='valid',
                input_shape=(1, 32, 32)))
        model.add(Activation('relu'))

        model.add(Convolution2D(64, 5, 5))
        model.add(Activation('relu'))

        model.add(Convolution2D(64, 5, 5))
        model.add(Activation('relu'))

        model.add(MaxPooling2D(pool_size=(3, 3)))
        model.add(Dropout(0.25))

        model.add(Flatten())
        model.add(Dense(400))
        model.add(Activation('relu'))
        model.add(Dropout(0.5))

        model.add(Dense(nb_class))
        model.add(Activation('softmax'))

        if weights_path:
            model.load_weights(weights_path)

        return model